# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zs2TPbqlrBYpOrIkI9_UrIXEDKWU2VdO
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import string

from google.colab import files
uploaded = files.upload()

import pandas as pd

df = pd.read_csv("PoetryFoundationData.csv")

# Data Preprocessing
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def clean_text(s: str) -> str:
    s = s.lower()
    # remove punctuation
    s = s.translate(str.maketrans('', '', string.punctuation))
    tokens = s.split()
    cleaned = []
    for t in tokens:
        if t in stop_words:
            continue
        t2 = lemmatizer.lemmatize(t)
        cleaned.append(t2)
    return " ".join(cleaned)

#Mean,Median,Mode

df['word_count'] = df['Poem'].str.split().str.len()

grouped = df.groupby('Poet')['word_count']
summary_per_Poet = grouped.agg(['mean', 'median', lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan])
summary_per_Poet = summary_per_Poet.rename(columns={'<lambda_0>': 'mode'})
print(summary_per_Poet.head())

# Visualisation
df['word_count'] = pd.to_numeric(df['word_count'], errors='coerce')
df_clean = df.dropna(subset=['word_count'])
top_authors = df_clean['Poet'].value_counts().nlargest(6).index
filtered_df = df_clean[df_clean['Poet'].isin(top_authors)]
plt.figure(figsize=(14, 8))
sns.set(style="whitegrid")
ax = sns.boxplot(x='Poet', y='word_count', data=filtered_df, palette="Set2", showfliers=False)
plt.show()

#Train/Test Split Data
class_counts = df['Poet'].value_counts()
valid_poets = class_counts[class_counts >= 2].index
filtered_df = df[df['Poet'].isin(valid_poets)]
X = filtered_df[['word_count']]
Y = filtered_df['Poet']
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42, stratify=Y)

# Decision Tree Classifier
clf = DecisionTreeClassifier(
    criterion='entropy',
    max_depth=25,
    min_samples_leaf=5,
    random_state=42)
clf.fit(X_train, Y_train)
Y_pred = clf.predict(X_test)
#Evaluation
print("\nDecision Tree Classifier Results:")
print("Accuracy:", accuracy_score(Y_test, Y_pred))
print("\nClassification Report:\n", classification_report(Y_test, Y_pred))

# Confusion matrix
cm = confusion_matrix(Y_test, Y_pred, labels=clf.classes_)
cm_df = pd.DataFrame(cm, index=clf.classes_, columns=clf.classes_)
print("\nConfusion Matrix:\n", cm_df)

# Tree Visualisation
clf = DecisionTreeClassifier(max_depth=5, random_state=42)
clf.fit(X_train, Y_train)

# Step 5: Plot the tree
plt.figure(figsize=(30, 20))
plot_tree(
    clf,
    feature_names=X.columns,
    class_names=clf.classes_,
    filled=True,
    fontsize=10
)
plt.title("Decision Tree Based on Word Count")
plt.show()